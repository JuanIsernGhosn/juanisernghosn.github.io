I"òA<p>The document presented below corresponds to the final report generated after the coral classification work of the <strong>RSMAS data set</strong>. This project requires the use of Deep Learning, in order to generate models that maximize the accuracy of the classification of other images of the same set that have not been seen by the Deep Learning model.</p>

<p>The library that has been used to develop the models presented in this practice is the Keras high-level neural network API, which implements a set of functionalities that facilitate the development of neural network architectures and their correct training and adjustment. Thus, the main approach used for the development of models and that, to a large extent, is also facilitated by Keras, has been the generation of models by means of <strong>Transfer Learning and Fine Tuning</strong>.</p>

<h2 id="rsmas-data-set">RSMAS data set</h2>

<p>There are currently eight open reference points used for the classification of underwater corals. The RSMAS set, one of the most recent and largest RGB datasets containing corals, has been used in this competition. This dataset is composed of images of parts of corals, which mainly capture the texture of the corals rather than the overall structure of the coral in question.
Thus, the RSMAS image set contains 766 images of size 256Ã—256. The images were collected by divers from the University of Miamiâ€™s Rosenstiel School of Marine and Atmospheric Science. These images were taken with different cameras in different locations and have been classified into 14 classes, which are labeled with the Latin names of the coral species. Several images of the data set are shown in Figure 1. For this project, the RSMAS image set has been divided as follows:</p>

<ul>
  <li>
    <p>A training subset, consisting of 620 images from the original set, with the following distribution by class:</p>

    <ul>
      <li>ACER: 88</li>
      <li>SALW: 62</li>
      <li>CNAT: 46</li>
      <li>DANT: 51</li>
      <li>DSTR: 20</li>
      <li>GORG: 48</li>
      <li>MALC: 18</li>
      <li>MCAV: 64</li>
      <li>MMEA: 44</li>
      <li>MONT: 23</li>
      <li>PALY: 26</li>
      <li>SPO: 71</li>
      <li>SSID: 30</li>
      <li>TUNI: 29</li>
    </ul>
  </li>
  <li>
    <p>A test subset, consisting of the remaining 146 images from the data set. This test subset has in turn been divided into two others:</p>

    <ul>
      <li>29% of it is used for public classification during the project, so that one can observe the modelâ€™s performance and get an idea of the potential with respect to the rest of the implemented models.</li>
      <li>The other 71% is used for the private classification, which is released once the project is over, so that it is possible to have an idea of the generalization capacity of the models generated on previously untested data.</li>
    </ul>
  </li>
</ul>

<h1 id="pre-processing-used">Pre-processing used</h1>

<p>In order to facilitate the learning of our model, several pre-processing tasks have been carried out on our images. Thus, in this section we present these tasks, emphasizing their relevance, why they have been proposed and how they have been developed.</p>

<h2 id="correction-of-the-luminosity-of-the-images">Correction of the luminosity of the images</h2>

<p>After carrying out a first visualization of the images, we noticed that in some classes there were certain images that, due to their capture conditions, made it difficult to visualize certain details and that therefore, did not allow to extract information that could be considered completely representative of the class in question, either because of the dim light with which the image had been taken or because of the excessive brightness present in the image.</p>

<p>Thus, in order to solve this problem, the intensity of the images was corrected. To do this, and at first, the colour space with which the images were loaded (originally in RGB) was changed to YCrCb, where the Y channel represents the luma or luminosity component of the image, while Cb and Cr are the chrominance components, which differ from blue and differ from red. This change of color space was done by one of the OpenCV functions and the line in question is shown below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">img_YCrCb</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2YCrCb</span><span class="p">)</span>
</code></pre></div></div>

<p>The technique used to improve the contrast of our images was the equalization of the histogram. This is a transformation that aims to obtain for an image a histogram with a uniform distribution. Specifically, we made use of an algorithm called CLAHE (Contrast Limited Adaptive Histogram Equalization) and its concrete implementation of OpenCV.</p>

<p>For this adaptive histogram equalization, the image is divided into small blocks (8x8 by default in OpenCV). Then, each of these blocks are equalized individually so that in a small area, the histogram would be limited to a small region. In addition, to avoid amplifying existing noise, a contrast limitation is applied, so that if in any of the cells of the histogram it is above the specified contrast limit (default 40 in OpenCV), these pixels are cut out and distributed evenly in other cells. The lines of code needed to carry out this preprocessing are shown below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">clahe</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">createCLAHE</span><span class="p">(</span><span class="n">clipLimit</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">tileGridSize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">cl1</span> <span class="o">=</span> <span class="n">clahe</span><span class="o">.</span><span class="nb">apply</span><span class="p">(</span><span class="n">img_YCrCb</span><span class="p">[:,:,</span><span class="mi">0</span><span class="p">])</span>
<span class="n">img_YCrCb</span><span class="p">[:,:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">cl1</span>
<span class="n">img_RGB</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">img_YCrCb</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_YCrCb2RGB</span><span class="p">)</span>
</code></pre></div></div>

<p>Figure 1 below shows one of the images of the training set before and after contrast correction:</p>

<p class="text-center">
  <img src="/assets/img/2019-04-23-dl-corals/hist.png" width="90%" title="Figure 1. Image before and after contrast correction" />
</p>

<h2 id="gaussian-blur">Gaussian blur</h2>

<p>As we have already commented in the previous section, after carrying out the intensity correction of the colours in the images, an increase in the detail of the textures of the corals and in the performance of our models could be appreciated. Although this is positive, since the image with which our model is trained is of higher quality, it is also known that this may result in an over-adjustment of the model, even more so in such a reduced data set.</p>

<p>Therefore, a Gaussian blur was applied to the images, so that the edges and details were not so defined and allowed a better generalization of the model. In reviewing the literature, it was concluded that this technique has proven to be very useful in pre-processing images in computer vision tasks, and specifically in neural networks and, finally, in Deep Learning. The Gaussian blur function used is the one implemented by OpenCV and the concrete line of code made is the following:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cv2</span><span class="o">.</span><span class="n">GaussianBlur</span><span class="p">(</span><span class="n">img_RGB</span><span class="p">,(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<p>Figure 2 below shows a picture of the training set before and after the Gaussian blur:</p>

<p class="text-center">
  <img src="/assets/img/2019-04-23-dl-corals/gauss.png" width="90%" title="Figure 2. Image before and after Gaussian blur." />
</p>

<h2 id="data-augmentation">Data augmentation</h2>

<p>First of all, one of the first approaches used has been to generate new images in a synthetic way by means of different transformations of the original images. This technique is commonly known as Data Augmentation and its use is quite common, especially in situations where the size of the dataset is very small, which makes it difficult for our model to describe the whole domain that underlies our classification problem and that, therefore, tends to over-adjust.</p>

<p>In the Keras library, the accomplishment of the Data Augmentation is relatively simple, as it has implemented an ImageDataGenerator object that, as its name indicates, dynamically generates images in the training process of the model, following the transformations indicated as parameters in its instantiation. The following example shows the instantiation of an ImageDataGenerator object with the parameters that have been used during the competition in the different deliveries:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">datagen</span> <span class="o">=</span> <span class="n">ImageDataGenerator</span><span class="p">(</span>
    <span class="n">featurewise_center</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
    <span class="n">featurewise_std_normalization</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
    <span class="n">rescale</span> <span class="o">=</span> <span class="mf">1.0</span><span class="o">/</span><span class="mi">255</span><span class="p">,</span>
    <span class="n">zoom_range</span> <span class="o">=</span> <span class="n">x</span><span class="p">,</span>
    <span class="n">rotation_range</span> <span class="o">=</span> <span class="n">y</span><span class="p">,</span>
    <span class="n">horizontal_flip</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
    <span class="n">validation_split</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="p">)</span>
</code></pre></div></div>

<p>Each of the different parameters, as well as the values they have been tested with, are described below:</p>

<ul>
  <li><strong>featurewise_center</strong>: Sets the average of the data to 0, subtracting the average of the variables at the data set level.</li>
  <li><strong>featurewise_std_normalization</strong>: Divides the data by the standard deviation of the variables at data set level.</li>
  <li><strong>rescale</strong>: With this parameter, each pixel value in the image is multiplied by a past constant. In our case, the times we have used this option have been to include the data of each image in an interval [0,1], for which it has been necessary to multiply these by the inverse of the maximum possible value in a RGB channel: 1/255.</li>
  <li><strong>zoom_range</strong>: It consists in making a random zoom in the image in a determined range. Thus, given x, each image is resized in the interval [1-x,1+x]. The zoom values tested have been 0.2, 0.3 and 0.4, obtaining the best results with the second value.</li>
  <li><strong>rotation_range</strong>: This transformation consists in randomly rotating the images in an angle that will be the maximum passed by parameter. Given a value of y, each image is rotated in an angle contained in the interval [0, y].</li>
  <li><strong>Horizontal_flip</strong>: It consists in flipping the images horizontally in a random way.</li>
  <li><strong>validation_split</strong>: Defines the percentage of the data set that will be used for validation after each period. Due to the size of the image set, it was set to 0.2 so that there would be several images of the same classes.</li>
</ul>

<p>Although in the previous object it allows to carry out a standardization of the training data when performing the Data Augmentation, in the same way, a normalization of the test data has to be done so that they are centered and scaled in the same way and that allows the predictions to be done correctly. Thus, this transformation on the test data has been done as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x_test</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_test</span> <span class="o">-</span> <span class="n">datagen</span><span class="o">.</span><span class="n">mean</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">datagen</span><span class="o">.</span><span class="n">std</span> <span class="o">+</span> <span class="mf">0.000001</span><span class="p">)</span>
</code></pre></div></div>

<p>Next, Figure 3 shows a set of images generated synthetically as a result of the application of the previously commented transformations on images to which their intensity was normalized and to which a Gaussian filter was applied.</p>

<p class="text-center">
  <img src="/assets/img/2019-04-23-dl-corals/aug.png" width="90%" title="Figure 3. Nine images generated by Data Augmentation (Keras)." />
</p>

<h1 id="models-used">Models used</h1>

<p>As stated in the Introduction section above, one of the advantages we have taken advantage of with Deep Learning techniques is the ability to do Transfer Learning. With this technique, what we achieve is to take advantage of the characteristics that the convolutional layers have extracted in another problem where the amount of data was higher, and to train the final layers for our problem. This technique is widely used and leads to very good results in a variety of problems.</p>

<p>To take advantage of this technique we have made use of three models widely used in the literature: VGG-16, Resnet-50 and Xception.</p>

<h2 id="vgg-16">VGG-16</h2>

<p>The VGG network was proposed by Simonyan and Zisserman in their 2014 article <em>Very Deep Convolutional Networks for Large Scale Image Recognition</em>.</p>

<p>This network is characterized by its simplicity. It consists of convolution layers with 3x3 kernels on top of each other. In addition, the dimensionality is reduced with a MaxPooling layer. Finally, two layers of 4096 neurons followed by a dense output layer with a Softmax activation function.</p>

<p>To make use of what we have learned in another data set it is necessary to add new layers on this architecture, which will be the ones we will train. We also tried to retrain some of the convolutional layers of the model but obtained worse results. The layers we have added are the following:</p>

<ul>
  <li><strong>Global Avarage Pooling</strong>: This layer has been used in recent years as an overfitting prevention technique. What it does is reduce the dimensions that are in the previous layer, letâ€™s call them (h x w x d) to (1 x 1 x d).  What is done is, for each filter size (h x w), an average to leave it in a single value. In this way we also reduce the number of network parameters.</li>
  <li><strong>Dropout</strong>: This layer is also a widely used one to reduce the over-adjustment. In each iteration a random percentage N of connections between the neurons of the dense layers is blocked, so that not all of them will connect to all of them. The ranges between which we moved were between 0.25 and 0.5.</li>
  <li><strong>Dense</strong>: A dense layer of N neurons These N neurons were chosen in the process of tuning the hyperparameters. The ranges we moved between were between 512 and 1024.</li>
  <li><strong>Dropout</strong>: Another dropout layer to prevent further over-tuning.</li>
  <li><strong>Dense</strong>: A dense layer of N neurons. These N neurons were chosen in the process of tuning the hyperparameters. The ranges we moved between were between 512 and 1024.</li>
  <li><strong>Dense</strong>: The output layer with 14 neurons one per class.</li>
</ul>
:ET