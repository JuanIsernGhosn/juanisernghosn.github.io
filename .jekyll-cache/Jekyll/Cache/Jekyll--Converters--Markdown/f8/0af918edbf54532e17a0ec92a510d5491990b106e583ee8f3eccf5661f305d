I"ò<p>The document presented below corresponds to the final report generated after the coral classification work of the <strong>RSMAS data set</strong>. This project requires the use of Deep Learning, in order to generate models that maximize the accuracy of the classification of other images of the same set that have not been seen by the Deep Learning model.</p>

<p>The library that has been used to develop the models presented in this practice is the Keras high-level neural network API, which implements a set of functionalities that facilitate the development of neural network architectures and their correct training and adjustment. Thus, the main approach used for the development of models and that, to a large extent, is also facilitated by Keras, has been the generation of models by means of <strong>Transfer Learning and Fine Tuning</strong>.</p>

<h2 id="rsmas-data-set">RSMAS data set</h2>

<p>There are currently eight open reference points used for the classification of underwater corals. The RSMAS set, one of the most recent and largest RGB datasets containing corals, has been used in this competition. This dataset is composed of images of parts of corals, which mainly capture the texture of the corals rather than the overall structure of the coral in question.
Thus, the RSMAS image set contains 766 images of size 256Ã—256. The images were collected by divers from the University of Miamiâ€™s Rosenstiel School of Marine and Atmospheric Science. These images were taken with different cameras in different locations and have been classified into 14 classes, which are labeled with the Latin names of the coral species. Several images of the data set are shown in Figure 1. For this project, the RSMAS image set has been divided as follows:</p>

<ul>
  <li>
    <p>A training subset, consisting of 620 images from the original set, with the following distribution by class:</p>

    <ul>
      <li>ACER: 88</li>
      <li>SALW: 62</li>
      <li>CNAT: 46</li>
      <li>DANT: 51</li>
      <li>DSTR: 20</li>
      <li>GORG: 48</li>
      <li>MALC: 18</li>
      <li>MCAV: 64</li>
      <li>MMEA: 44</li>
      <li>MONT: 23</li>
      <li>PALY: 26</li>
      <li>SPO: 71</li>
      <li>SSID: 30</li>
      <li>TUNI: 29</li>
    </ul>
  </li>
  <li>
    <p>A test subset, consisting of the remaining 146 images from the data set. This test subset has in turn been divided into two others:</p>

    <ul>
      <li>29% of it is used for public classification during the project, so that one can observe the modelâ€™s performance and get an idea of the potential with respect to the rest of the implemented models.</li>
      <li>The other 71% is used for the private classification, which is released once the project is over, so that it is possible to have an idea of the generalization capacity of the models generated on previously untested data.</li>
    </ul>
  </li>
</ul>

<h1 id="pre-processing-used">Pre-processing used</h1>

<p>In order to facilitate the learning of our model, several pre-processing tasks have been carried out on our images. Thus, in this section we present these tasks, emphasizing their relevance, why they have been proposed and how they have been developed.</p>

<h2 id="correction-of-the-luminosity-of-the-images">Correction of the luminosity of the images</h2>

<p>After carrying out a first visualization of the images, we noticed that in some classes there were certain images that, due to their capture conditions, made it difficult to visualize certain details and that therefore, did not allow to extract information that could be considered completely representative of the class in question, either because of the dim light with which the image had been taken or because of the excessive brightness present in the image.</p>

<p>Thus, in order to solve this problem, the intensity of the images was corrected. To do this, and at first, the colour space with which the images were loaded (originally in RGB) was changed to YCrCb, where the Y channel represents the luma or luminosity component of the image, while Cb and Cr are the chrominance components, which differ from blue and differ from red. This change of color space was done by one of the OpenCV functions and the line in question is shown below:</p>

<p>Translated with www.DeepL.com/Translator (free version)</p>
:ET